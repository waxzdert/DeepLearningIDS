{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {\n",
    "    'Train':'C:\\\\Users\\\\MaxWu\\\\Documents\\\\GitHub\\\\DeepLearningIDS\\\\Datasets\\\\KDDTrain+.csv',\n",
    "    'Test':'C:\\\\Users\\\\MaxWu\\\\Documents\\\\GitHub\\\\DeepLearningIDS\\\\Datasets\\\\KDDTest+.csv',\n",
    "    'Minus21':'C:\\\\Users\\\\MaxWu\\\\Documents\\\\GitHub\\\\DeepLearningIDS\\\\Datasets\\\\KDDTest-21.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#計算資料的特徵中有多少獨立項\n",
    "#print(raw_data.groupby('protocol_type').ngroups)\n",
    "#print(raw_data.groupby('Service').ngroups)\n",
    "#print(raw_data.groupby('flag').ngroups)\n",
    "\n",
    "#================== OneHot Encoding ==================\n",
    "def one_hot(in_df):\n",
    "\n",
    "    # Convert some nonnumeric features into numeric form\n",
    "    # Example: protocol_type, service and flag\n",
    "\n",
    "    #將prorocol type 轉換為數值, dim: 41->44, (125973, 44)\n",
    "    oh_df = pd.get_dummies(data=in_df, columns=['protocol_type'])\n",
    "    #將Service轉換為數值, dim: 44->112, (125973, 112)\n",
    "    oh_df = pd.get_dummies(data=oh_df, columns=['Service'])\n",
    "    #將flag轉換為數值, dim: 112->122,(125973, 122)\n",
    "    oh_df = pd.get_dummies(data=oh_df, columns=['flag'])\n",
    "    return oh_df\n",
    "\n",
    "#================== Normalization ==================\n",
    "def fir_norm(in_df):\n",
    "    # Normalize the data which difference between maximum and minimum values\n",
    "    # such as : ‘duration[0,58329]’,‘src_bytes[0,1.3 × 109]’ and ‘dst_bytes\n",
    "    # Logarithmic scaling method for scaling to obtain the ranges\n",
    "    # (x,y) -> (log(x),log(y))\n",
    "    for i in range(len(in_df['Duration'])):\n",
    "        if (in_df.loc[i,('Duration')]) == 0:\n",
    "            in_df.loc[i,('Duration')] = 0\n",
    "        else:\n",
    "            in_df.loc[i,('Duration')] = round(math.log(in_df.loc[i,('Duration')],10), 2)\n",
    "\n",
    "    for i in range(len(in_df['src_bytes'])):\n",
    "        if (in_df.loc[i,('src_bytes')]) == 0:\n",
    "            in_df.loc[i,('src_bytes')] = 0\n",
    "        else:\n",
    "            in_df.loc[i,('src_bytes')] = round(math.log(in_df.loc[i,('src_bytes')],10), 2)\n",
    "\n",
    "    for i in range(len(in_df['dst_bytes'])):\n",
    "        if (in_df.loc[i,('dst_bytes')]) == 0:\n",
    "            in_df.loc[i,('dst_bytes')] = 0\n",
    "        else:\n",
    "            in_df.loc[i,('dst_bytes')] = round(math.log(in_df.loc[i,('dst_bytes')],10), 2)\n",
    "\n",
    "def sec_norm(in_df):\n",
    "    # Normalize all the data in the dataframe\n",
    "    # let the data in the frame can \n",
    "    # new Xi = ((old Xi)-min)/(Max-min)\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    # remove the feature with string which can't normalize\n",
    "    temp_data = in_df.drop('result', axis=1)\n",
    "\n",
    "    # let data transform the type from dataframe to numpy array\n",
    "    temp_data = temp_data.values\n",
    "\n",
    "    # Define a scaler that will feed nraw data afterward\n",
    "    # The scaler will normalize the data into new Xi = ((old Xi)-min)/(Max-min)\n",
    "    # And it's output will range from 0 to 1.\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "    # feed the data to the scaler\n",
    "    temp_data = scaler.fit_transform(temp_data)\n",
    "\n",
    "    # Transfer the numpy array to dataframe\n",
    "    temp_data = pd.DataFrame(temp_data)\n",
    "\n",
    "    # merge the feature which remove first\n",
    "    temp_data = temp_data.join(in_df[['result']])\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "def ren_idx(new_df, old_df):\n",
    "    # This function main use to rename the index of the dataset\n",
    "    \n",
    "    # Create a new index set\n",
    "    temp_columns =[]\n",
    "    for i in range(1,len(old_df.columns)):\n",
    "        temp_columns.append(old_df.columns[i])\n",
    "\n",
    "    temp_columns.append('results')\n",
    "\n",
    "    in_df.columns = temp_columns\n",
    "\n",
    "def label_trans(in_df):\n",
    "    # 1 represent the traffic is an attack\n",
    "    # 0 represent the traffic is a normal traffic\n",
    "    for i in range(len(in_df[122])):\n",
    "        if (in_df.loc[i,122]) != 'normal':\n",
    "           in_df.loc[i,122] = 1\n",
    "        else:\n",
    "            in_df.loc[i,122] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(raw_data, idx):\n",
    "    OneHotData = one_hot(raw_data)\n",
    "    fir_norm(OneHotData)\n",
    "    Processed_Data = sec_norm(OneHotData)\n",
    "    label_trans(Processed_Data)\n",
    "    Processed_Data.to_csv('Processed_Data_%s.csv' % (idx),index=False,index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.read_csv(file_dict['Train'])\n",
    "#execute(train_data, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(file_dict['Train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotData = one_hot(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fir_norm(OneHotData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Data = sec_norm(OneHotData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-094d510e3376>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mren_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mProcessed_Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-2ad1617e9190>\u001b[0m in \u001b[0;36mren_idx\u001b[1;34m(new_df, old_df)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mtemp_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0min_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlabel_trans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'in_df' is not defined"
     ]
    }
   ],
   "source": [
    "ren_idx(Processed_Data, OneHotData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
